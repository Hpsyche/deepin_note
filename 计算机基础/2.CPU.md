## 指令集

对于每个CPU，其都有一套自己可以执行的专门的指令集（这部分指令由CPU提供）。

正式因为不同CPU架构的指令集不同，使得X86处理器不能执行ARM程序，ARM程序也不能执行X86程序（Inter和AMD都使用x86指令集，手机绝大多数使用ARM指令集）。

注意：指令集具有软硬件层次之分：硬件指令集是硬件层次上由CPU自身提供的可执行的指令集合。软件指令集是指语言程序库所提供的指令，只要安装了该语言的程序库，指令就可以执行。

## 寄存器

当程序要执行的部分被装载到内存中后，CPU要从内存中取出指令，然后指令解码（以便知道类型和操作数，简单的理解为：CPU要知道这是什么指令），然后执行该指令。再然后取下一个指令，重复以上操作。

由于CPU访问内存已得到指令或数据的时间要比执行指令花费的时间多得多，因此在CPU内部提供了一些来保存关键变量、临时数据等信息的通用寄存器。

所以，CPU需要提供一些特定的指令，使得可以从内存中读取数据存入寄存器以及将寄存器数据存入内存。

### 常见寄存器

IP寄存器：用来指向当前执行指令的下一条指令（非Inter厂家将IP寄存器称为PC寄存器：即程序计数器），指令取出后，就会更新该寄存器指向下一条指令；

CS寄存器：代码段寄存器，一般用于存放代码；

通常CS和IP配合使用，用于处理下一条执行的代码。

DS寄存器：数据段寄存器，一般用于存放数据；

* 一段内存定义为一个段，用一个段地址指示段，用偏移地址访问段内的单元；
* 用一个段存放数据，将它定义为“数据段”。用一段存放代码，将它定义为“代码段”，用一段当做栈，将它定义为“栈段”。
* 对于数据段，将它的段地址放到DS中，用mov、add、sub等访问内存单元的指令时，CPU就将我们定义的数据段中的内容当做数据来访问。对于代码段，将它的段地址放在CS中，将段中第一条指令的偏移地址放在IP中，这样CPU就将执行我们定义的的代码段中的指令。
* CPU将内存中的某段内容当做代码，是因为CS:IP指向哪里。

#### 存储寄存器

MAR：保存将要被访问数据在内存中哪个地址，保存的是地址值

MDR：保存从内存读取进来的数据或将要被写入内存的数据，保存的是数据值

IR：保存当前正在执行的指令

* 存储器地址寄存器（MAR）和存储器数据寄存器（MDR）是主存和CPU之间的接口。MAR可以接收由程序计数器（PC）的指令地址或来自运算器的操作数的地址，以确定要访问的单元。MDR是向主存写入数据或从主存读出数据的缓冲部件。MAR和MDR从功能上看属于主存，但通常放在CPU内。

AC寄存器：累加寄存器，当运算器的算术逻辑单元(ALU)执行算术或逻辑运算时，为ALU提供一个工作区。累加寄存器暂时存放ALU运算的结果信息。

* ALU：算数逻辑单元，CPU中的关键部件，用来实现指令所指定的各种算术和逻辑运算操作。
* CU：控制器，负责将存储器中的数据送到ALU中去做运算，并将运算结果存回到存储器中

![](/media/hpsyche/_dde_data/note/计算机基础/pict/2-7.png)

跟着上图，以取数相加指令为例：

取指：首先指令地址在PC中，通过片内总线然后送到MAR寄存器中，MAR通过与内存管理器交互，将MAR指令下那条地址的指令取出来放到MDR寄存器中。接着送到IR寄存器（IR：存储当前指令）；

解码：将指令的操作码部分送给CU，由CU负责解码；

执行指令：因为是取数相加指令， 首先进行取数操作，在CU帮助下， 将取数指令的地址码送到MAR中， 然后通过与内存控制器交互，从内存中取出该地址下的数据到MDR；由于需要进行逻辑运算，通过CU控制，将两个MDR寄存器（保存了来自内存的两个数据）中的值拷贝到ALU中，然后ALU进行加法操作（中间可能使用到AC寄存器），计算完毕后将结果拷贝到MDR寄存器，最后写入回内存。

### PSW

在CPU中，有着PSW寄存器，表示程序状态字，这个寄存器中保存了一些控制位，比如CPU的优先级、CPU的工作模式（用户态、内核态）。

在CPU进行进程切换的时候，需要将寄存器中和当前进程有关的状态数据写入内存对应的位置(内核中该进程的栈空间)保存起来，**当切换回该进程时，需要从内存中拷贝回寄存器中**。即上下文切换时，需要保护现场和恢复现场。

内核态的CPU，可以执行指令集中的所有指令，并使用硬件的所有功能。

用户态的CPU，只允许执行指令集中的部分指令。一般而言，**IO相关和把内存保护相关的所有执行在用户态下都是被禁止的**，此外其它一些特权指令也是被禁止的，比如用户态下不能将PSW的模式设置控制位设置成内核态。

用户态CPU想要执行特权操作，需要发起系统调用来请求内核帮忙完成对应的操作。其实是在发起系统调用后，CPU会执行trap指令陷入(trap)到内核。当特权操作完成后，需要执行一个指令让CPU返回到用户态。

除了系统调用会陷入内核，更多的是硬件会引起trap行为陷入内核，使得CPU控制权可以回到操作系统，以便操作系统去决定如何处理硬件异常。

### CPU与多核

* CPU的物理个数由主板上的插槽数量决定，每个CPU可以有多核心，每核心可能会有多线程。

  多核CPU的每核(每核都是一个小芯片)，在OS看来都是一个独立的CPU。

  对于超线程CPU来说，每核CPU可以有多个线程(数量是两个，比如1核双线程，2核4线程，4核8线程)，每个线程都是一个虚拟的逻辑CPU(比如windows下是以逻辑处理器的名称称呼的)，而每个线程在OS看来也是独立的CPU。

  多线程的CPU在能力上，比非多线程的CPU核心要更强，但每个线程不足以与独立的CPU核心能力相比较。

* **每核上的多线程CPU都共享该核的CPU资源**。

  例如，假设每核CPU都只有一个"发动机"资源，那么线程1这个虚拟CPU使用了这个"发动机"后，线程2就没法使用，只能等待。

  所以，超线程技术的主要目的是为了增加流水线(参见前文对流水线的解释)上更多个独立的指令，这样线程1和线程2在流水线上就尽量不会争抢该核CPU资源。所以，超线程技术利用了superscalar(超标量)架构的优点。

* **多线程意味着每核可以有多个线程的状态**。比如某核的线程1空闲，线程2运行。

  多线程没有提供真正意义上的并行处理，每核CPU在某一时刻仍然只能运行一个进程，因为线程1和线程2是共享某核CPU资源的。

  可以简单的认为每核CPU在独立执行进程的能力上，有一个资源是唯一的，线程1获取了该资源，线程2就没法获取。

  但是，线程1和线程2在很多方面上是可以并行执行的。比如可以并行取指、并行解码、并行执行指令等。

  所以虽然单核在同一时间只能执行一个进程，但线程1和线程2可以互相帮助，加速进程的执行。

  并且，如果线程1在某一时刻获取了该核执行进程的能力，假设此刻该进程发出了IO请求，于是线程1掌握的执行进程的能力，就可以被线程2获取，即切换到线程2。这是在执行线程间的切换，是非常轻量级的。

* **每核心都有一个自己的L1缓存。**

  L1缓存分两种：L1指令缓存(L1-icache)和L1数据缓存(L1-dcache)。L1指令缓存用来存放已解码指令，L1数据缓存用来放访问非常频繁的数据。

  L2缓存用来存放近期使用过的内存数据。更严格地说，存放的是很可能将来会被CPU使用的数据。

  多数多核CPU的各核都各自拥有一个L2缓存，但也有多核共享L2缓存的设计。无论如何，L1是各核私有的(但对某核内的多线程是共享的)。

## 进程、线程的引入

在linux中，进程控制块即PCB的结构为task_struct，我们以linux2.6为例，其源码如下：

```c
struct task_struct {
    //表示进程当前运行状态
    //volatile避免了读取到缓存在寄存器中的脏数据，而是直接从内存中取
    //可以看到state基本有三种，但大于0还分为很多不同的状态
    //#define TASK_RUNNING		0
    //#define TASK_INTERRUPTIBLE	1
    //#define TASK_UNINTERRUPTIBLE	2
    //#define TASK_STOPPED		4
    //#define TASK_TRACED		8
    //#define EXIT_ZOMBIE		16
    //#define EXIT_DEAD		32
	volatile long state;	/* -1 unrunnable, 0 runnable, >0 stopped */
    //这个结构体保存了进程描述符中中频繁访问和需要快速访问的字段，内核依赖于该数据结构来获得当前进程的描述符。通过源码可以看到，该struct内拥有指向task_struct的指针
	struct thread_info *thread_info;
    
	atomic_t usage;
    //进程标志，描述进程当前的状态（不是运行状态），如：PF_SUPERPRIV表示进程拥有超级用户特权。。
	unsigned long flags;	/* per process flags, defined below */
    //系统调用跟踪
	unsigned long ptrace;
	//内核锁标志（判断是否被上锁）
	int lock_depth;		/* Lock depth */
	//进程优先级
	int prio, static_prio;
	struct list_head run_list;
    //进程调度队列
	prio_array_t *array;
	//进程平均等待时间
	unsigned long sleep_avg;
    //timestamp:进程最近插入运行队列的时间或最近一次进程切换的时间
    //last_ran:最近一次替换本进程的进程切换时间
	unsigned long long timestamp, last_ran;
    //进程被唤醒时所使用的条件代码，就是从什么状态被唤醒的。
	int activated;
	//进程的调度类型
	unsigned long policy;
	cpumask_t cpus_allowed;
    //time_slice:进程的剩余时间片
    //first_time_slice:创建后首次获取的时间片，为1表示当前的时间片是从父进程分来的
	unsigned int time_slice, first_time_slice;

#ifdef CONFIG_SCHEDSTATS
	struct sched_info sched_info;
#endif

	struct list_head tasks;
	/*
	 * ptrace_list/ptrace_children forms the list of my children
	 * that were stolen by a ptracer.
	 */
	struct list_head ptrace_children;
	struct list_head ptrace_list;
	//mm：内存描述符，其下有程地址空间下的虚拟内存信息
    //actvie_mm：内核线程所借用的地址空间
	struct mm_struct *mm,active_mm;

/* task state */
	struct linux_binfmt *binfmt;
    //进程的退出状态，大于0表示僵死
	long exit_state;
    //exit_code:存放进程的退出码
    //exit_signal:当进程退出时发给父进程的信号，如果是轻量级进程为-1
	int exit_code, exit_signal;
	int pdeath_signal;  /*  The signal sent when the parent dies  */
	/* ??? */
	unsigned long personality;
    //记录是否执行execve系统调用
	unsigned did_exec:1;
    //进程id
	pid_t pid;
    //所在线程组领头进程的PID
	pid_t tgid;
	/* 
	 * pointers to (original) parent process, youngest child, younger sibling,
	 * older sibling, respectively.  (p->father can be replaced with 
	 * p->parent->pid)
	 */
    //
	struct task_struct *real_parent; /* real parent process (when being debugged) */
    //指向P的当前父进程
	struct task_struct *parent;	/* parent process */
	/*
	 * children/sibling forms the list of my children plus the
	 * tasks I'm ptracing.
	 */
    //链表的头部，链表中所有元素都是P创建的子进程
	struct list_head children;	/* list of my children */
	//兄弟进程之间相连接的链表
    struct list_head sibling;	/* linkage in my parent's children list */
	//P所在进程组的领头进程
    struct task_struct *group_leader;	/* threadgroup leader */
	//每个进程有四个PID，把这四个PID挂到PID HASH表里的不同位置，这样从PID到task就很快了
	/* PID/PID hash table linkage. */
	struct pid pids[PIDTYPE_MAX];
    //为vfork()用来等待子进程的队列
	struct completion *vfork_done;		/* for vfork() */
	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */
	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */
	//进程的实时优先级
	unsigned long rt_priority;
    //以下为一些时间与定时信息
	unsigned long it_real_value, it_real_incr;
	cputime_t it_virt_value, it_virt_incr;
	cputime_t it_prof_value, it_prof_incr;
	struct timer_list real_timer;
	cputime_t utime, stime;
	unsigned long nvcsw, nivcsw; /* context switch counts */
	struct timespec start_time;
/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */
	unsigned long min_flt, maj_flt;
/* process credentials */
	uid_t uid,euid,suid,fsuid;
	gid_t gid,egid,sgid,fsgid;
	struct group_info *group_info;
	kernel_cap_t   cap_effective, cap_inheritable, cap_permitted;
	unsigned keep_capabilities:1;
	struct user_struct *user;
#ifdef CONFIG_KEYS
	struct key *session_keyring;	/* keyring inherited over fork */
	struct key *process_keyring;	/* keyring private to this process (CLONE_THREAD) */
	struct key *thread_keyring;	/* keyring private to this thread */
#endif
	int oomkilladj; /* OOM kill score adjustment (bit shift). */
	char comm[TASK_COMM_LEN];
/* file system info */
	int link_count, total_link_count;
/* ipc stuff */
	struct sysv_sem sysvsem;
/* CPU-specific state of this task */
	struct thread_struct thread;
/* filesystem information */
    //进程的可执行映象所在的文件系统
	struct fs_struct *fs;
/* open file information */
    //进程打开的文件
	struct files_struct *files;
/* namespace */
	struct namespace *namespace;
/* signal handlers */
	struct signal_struct *signal;
	struct sighand_struct *sighand;

	sigset_t blocked, real_blocked;
	struct sigpending pending;

	unsigned long sas_ss_sp;
	size_t sas_ss_size;
	int (*notifier)(void *priv);
	void *notifier_data;
	sigset_t *notifier_mask;
	
	void *security;
	struct audit_context *audit_context;

/* Thread group tracking */
   	u32 parent_exec_id;
   	u32 self_exec_id;
/* Protection of (de-)allocation: mm, files, fs, tty, keyrings */
	spinlock_t alloc_lock;
/* Protection of proc_dentry: nesting proc_lock, dcache_lock, write_lock_irq(&tasklist_lock); */
	spinlock_t proc_lock;
/* context-switch lock */
	spinlock_t switch_lock;

/* journalling filesystem info */
	void *journal_info;

/* VM state */
	struct reclaim_state *reclaim_state;

	struct dentry *proc_dentry;
	struct backing_dev_info *backing_dev_info;

	struct io_context *io_context;

	unsigned long ptrace_message;
	siginfo_t *last_siginfo; /* For ptrace use.  */
/*
 * current io wait handle: wait queue entry to use for io waits
 * If this thread is processing aio, this points at the waitqueue
 * inside the currently handled kiocb. It may be NULL (i.e. default
 * to a stack based synchronous wait) if its doing sync IO.
 */
	wait_queue_t *io_wait;
/* i/o counters(bytes read/written, #syscalls */
	u64 rchar, wchar, syscr, syscw;
#if defined(CONFIG_BSD_PROCESS_ACCT)
	u64 acct_rss_mem1;	/* accumulated rss usage */
	u64 acct_vm_mem1;	/* accumulated virtual memory usage */
	clock_t acct_stimexpd;	/* clock_t-converted stime since last update */
#endif
#ifdef CONFIG_NUMA
  	struct mempolicy *mempolicy;
	short il_next;
#endif
};
```

可以看到，task_struct很复杂，其包含了进程状态、内存、调度、文件系统、时间分配等各种信息，上面我也只是给出了部分的注释，抓住重点的task_struct，我们进一步查看其源码：

```c
struct mm_struct {
    //指向线性区对象的链表头
	struct vm_area_struct * mmap;		/* list of VMAs */
    //指向线性区对象的红黑树
	struct rb_root mm_rb;
    //指向最后一个引用的线性区对象
	struct vm_area_struct * mmap_cache;	/* last find_vma result */
    //在进程地址空间中搜索有效线性地址区间的方法
	unsigned long (*get_unmapped_area) (struct file *filp,
				unsigned long addr, unsigned long len,
				unsigned long pgoff, unsigned long flags);
    //释放线性区时调用的方法
	void (*unmap_area) (struct vm_area_struct *area);
    // 标识第一个分配的匿名线性区或者是文件内存映射的线性地址
	unsigned long mmap_base;		/* base of mmap area */
    //内核从这个地址开始搜索进程地址空间中线性地址的空闲区间
	unsigned long free_area_cache;		/* first hole */
    //指向页表
	pgd_t * pgd;
	atomic_t mm_users;			/* How many users with user space? */
	atomic_t mm_count;			/* How many references to "struct mm_struct" (users count as 1) */
	int map_count;				/* number of VMAs */
	struct rw_semaphore mmap_sem;
	//线性区的自旋锁和页表的自旋锁
    spinlock_t page_table_lock;		/* Protects page tables, mm->rss, mm->anon_rss */

	struct list_head mmlist;		/* List of maybe swapped mm's.  These are globally strung
						 * together off init_mm.mmlist, and are protected
						 * by mmlist_lock
						 */

    //各个片段的起始地址和终止地址
	unsigned long start_code, end_code, start_data, end_data;
	unsigned long start_brk, brk, start_stack;
	unsigned long arg_start, arg_end, env_start, env_end;
	unsigned long rss, anon_rss, total_vm, locked_vm, shared_vm;
	unsigned long exec_vm, stack_vm, reserved_vm, def_flags, nr_ptes;

	unsigned long saved_auxv[42]; /* for /proc/PID/auxv */

	unsigned dumpable:1;
	cpumask_t cpu_vm_mask;

	/* Architecture-specific MM context */
	mm_context_t context;

	/* Token based thrashing protection. */
	unsigned long swap_token_time;
	char recent_pagein;

	/* coredumping support */
	int core_waiters;
	struct completion *core_startup_done, core_done;

	/* aio bits */
	rwlock_t		ioctx_list_lock;
	struct kioctx		*ioctx_list;

	struct kioctx		default_kioctx;

	unsigned long hiwater_rss;	/* High-water RSS usage */
	unsigned long hiwater_vm;	/* High-water virtual memory usage */
};
```

其中这里要讲到的字段为mmap，其指向线性区对象的链表头，而对于pgd，其指向该进程的页表。

```tex
在地址空间中，mmap为地址空间的内存区域（用vm_area_struct结构来表示）链表，mm_rb用红黑树来存储，链表表示起来更加方便，红黑树表示起来更加方便查找。区别是，当虚拟区较少的时候，这个时候采用单链表，由mmap指向这个链表，当虚拟区多时此时采用红黑树的结构，由mm_rb指向这棵红黑树。这样就可以在大量数据的时候效率更高。所有的mm_struct结构体通过自身的mm_list域链接在一个双向链表上，该链表的首元素是init_mm内存描述符，代表init进程的地址空间。
```

对于mmap指向的vm_area_struct，我们继续深入源码：

```c
struct vm_area_struct {
    //指向vm_mm
	struct mm_struct * vm_mm;	/* The address space we belong to. */
	//该虚拟内存空间的首地址
    unsigned long vm_start;		/* Our start address within vm_mm. */
	//该虚拟内存空间的尾地址
    unsigned long vm_end;		/* The first byte after our end address
					   within vm_mm. */

    //VMA链表的下一个成员
	/* linked list of VM areas per task, sorted by address */
	struct vm_area_struct *vm_next;

	pgprot_t vm_page_prot;		/* Access permissions of this VMA. */
	//保存VMA标志位
    unsigned long vm_flags;		/* Flags, listed below. */
	//将本VMA作为一个节点加入到红黑树中
	struct rb_node vm_rb;
    ...
}
```

可以看出，虚拟内存即为由一个个vm_area_struct结构体，通过链表组装起来的空间。

因此，个人尝试模拟了linux中虚拟内存的结构图，如下：

![](/media/hpsyche/_dde_data/note/计算机基础/pict/2-10.png)

当实际上，并非每个vm_area_struct对应指向内存的每个段（每个segment可能由多个VMA组成），对于vm_area_struct，其描述的是一**段连续的、具有相同访问属性的虚存空间，该虚存空间的大小为物理内存页面的整数倍**。

### 文件描述符

task_struct中还有：files 指针指向一个数组，这个数组里装着所有该进程打开的文件的指针。（链表或红黑树结构）

先说 files ，它是一个文件指针数组。一般来说，一个进程会从 files[0] 读取输入，将输出写入 files[1] ，将错误信息写入 files[2] 。

举个例子，以我们的角度 C 语言的 printf 函数是向命令行打印字符，但是从进程的角度来看，就是向 files[1] 写入数据;同理， scanf 函数就是进程试图从 files[0] 这个文件中读取数据。

每个进程被创建时， files 的前三位被填入默认值，分别指向标准输入流、标准输出流、标准错误流。我们常说的「文件描述符」就是指这个文件指针数组的索引 ，所以程序的文件描述符默认情况下 0 是输入，1 是输出，2 是错误。

我们可以画一幅图：

![](/media/hpsyche/_dde_data/note/计算机基础/pict/2-12.jpeg)

对于一般的计算机，输入流是键盘，输出流是显示器，错误流也是显示器，所以现在这个进程和内核连了三根线。因为硬件都是由内核管理的，我们的进程需要通过「系统调用」让内核进程访问硬件资源。

PS：不要忘了，Linux 中一切都被抽象成文件，设备也是文件，可以进行读和写。

如果我们写的程序需要其他资源，比如打开一个文件进行读写，这也很简单，进行系统调用，让内核把文件打开，这个文件就会被放到 files 的第 4 个位置，对应文件描述符 3：

![](/media/hpsyche/_dde_data/note/计算机基础/pict/2-13.jpeg)

明白了这个原理， 输入重定向 就很好理解了，程序想读取数据的时候就会去 files[0] 读取，所以我们只要把 files[0] 指向一个文件，那么程序就会从这个文件中读取数据，而不是从键盘：

![](/media/hpsyche/_dde_data/note/计算机基础/pict/2-14.jpeg)

同理， 输出重定向 就是把 files[1] 指向一个文件，那么程序的输出就不会写入到显示器，而是写入到这个文件中：

![](/media/hpsyche/_dde_data/note/计算机基础/pict/2-15.jpeg)

错误重定向也是一样的，就不再赘述。

管道符其实也是异曲同工，把一个进程的输出流和另一个进程的输入流接起一条「管道」，数据就在其中传递，不得不说这种设计思想真的很巧妙：

![](/media/hpsyche/_dde_data/note/计算机基础/pict/2-16.jpeg)

到这里，你可能也看出「Linux 中一切皆文件」设计思路的高明了，不管是设备、另一个进程、socket 套接字还是真正的文件，全部都可以读写，统一装进一个简单的 files 数组，进程通过简单的文件描述符访问相应资源，具体细节交于操作系统，有效解耦，优美高效。

### linux fork子进程

在linux中fork子进程结构大体如下：

![](/media/hpsyche/_dde_data/note/计算机基础/pict/fork子进程.png)

可以看到，对于父子进程的fork来说，子进程会拷贝父进程的数据空间、堆和栈空间（实际上是采用写时复制技术），二者共享代码段。所以在子进程中修改全局变量（局部变量，分配在堆上的内存同样也是）后，父进程的相同的全局变量不会改变。

### 线程

linux中线程和进程其实是一样，属于轻量级进程，在linux中线程的大致结构如下（创建线程（pthread_create）：

![](/media/hpsyche/_dde_data/note/计算机基础/pict/linux线程.png)

可以看到：fork之后，轻量级进程会与父进程共享区域，除了在栈空间有所差异之外，其他并无差别。

当然，必须要说明的是，**只有 Linux 系统将线程看做共享数据的进程**，不对其做特殊看待，其他的很多操作系统是对线程和进程区别对待的，线程有其特有的数据结构，我个人认为不如 Linux 的这种设计简洁，增加了系统的复杂度。

在 Linux 中新建线程和进程的效率都是很高的，对于新建进程时内存区域拷贝的问题，Linux 采用了 copy-on-write 的策略优化，也就是并不真正复制父进程的内存空间，而是等到需要写操作时才去复制。**所以 Linux 中新建进程和新建线程都是很迅速的**。

### volatile

CPU只执行代码等操作时，分为了取指、解码、执行指令等指令，CPU通过流水线的设计，使用多个进程可以并发处理，而在其中，可能会出现loadload\loadstore\storestore\storeload等内存重排序的现象，volatile就是用来禁止此些重排序操作的（编译器级别），多于多CPU，volatile无法禁止，所以一般不在cpp使用此关键字。

在java中，编译器和cpu级别的重排序都禁止了。



